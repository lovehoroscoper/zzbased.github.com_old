
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="author" content="vincentyao" />
    <title>Em算法随笔</title>

    <link rel="stylesheet" href="/assets/themes/dinky/css/styles.css">
    <link rel="stylesheet" href="/assets/themes/dinky/css/pygment_trac.css">
    <script src="/assets/themes/dinky/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    



  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header"><a href="/">100的技术博客</a></h1>
        <p class="header">广告、算法与金融</p>
        <ul>
          
          
          


  
    
      
      	
      	<li><a href="/archive.html">Archive</a></li>
      	
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/categories.html">Categories</a></li>
      	
      
    
  
    
  
    
      
    
  
    
      
      	
      	<li><a href="/pages.html">Pages</a></li>
      	
      
    
  
    
      
    
  
    
      
    
  
    
      
      	
      	<li><a href="/tags.html">Tags</a></li>
      	
      
    
  



        </ul>
        
        
        <div class="misc vcard">
          <h4>about</h4>
          <ul>
            
            <li class="contact"><address><span class="author fn n">vincentyao</span> - <span class="fn email">zerobased@foxmail.com</span></address></li>
            
            
            <li class="github"><a href="http://github.com/zzbased/" rel="me">github.com/zzbased</a></li>
            
            
            <li class="twitter"><a href="http://weibo.com/zerobased/" rel="me">weibo.com/zerobased</a></li>
            
            
            <li class="twitter"><a href="http://twitter.com/callyling/" rel="me">twitter.com/callyling</a></li>
            
            
            
            
            
          </ul>
        </div><!-- misc -->
        
      </header>

      <section>
        
<section>
  <h1>Em算法随笔</h1>

  
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="em">EM算法随笔</h1>

<h2 id="em-overview">EM overview</h2>

<p>The EM algorithm belongs to a broader class of <strong>alternating minimization algorithms</strong>.</p>

<p>EM is one such hill-climbing algorithm that converges to a local maximum of the likelihood surface.</p>

<p>As the name suggests, the EM algorithm alternates between an expectation and a maximization step. The “E step” finds a lower bound that is equal to the log-likelihood function at the current parameter estimate θ_k. The “M step” generates the next estimate θ_k+1 as the parameter that maximizes this greatest lower bound.</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_image1.png" alt="" /></p>

<p>x是隐变量。下面变换中用到了著名的Jensen不等式</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_q_function.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_formula2.png" alt="" /></p>

<p>参考文献：</p>

<ul>
  <li>
    <p>Zhai chengxiang老师的经典EM note：<a href="http://www.cs.ust.hk/~qyang/Teaching/537/PPT/em-note.pdf">A Note on the Expectation-Maximization (EM) Algorithm</a></p>
  </li>
  <li>
    <p>Shane M. Haas的<a href="http://www.mit.edu/~6.454/www_fall_2002/shaas/summary.pdf">The Expectation-Maximization and Alternating Minimization Algorithms</a></p>
  </li>
</ul>

<h2 id="em-1">EM算法细节</h2>

<h3 id="jensen">Jensen不等式</h3>
<p>Jensen不等式表述如下：</p>

<p>如果f是凸函数，X是随机变量，那么：E[f(X)]&gt;=f(E[X])</p>

<p>特别地，如果f是严格凸函数，当且仅当X是常量时，上式取等号。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/jensen_inequality.jpg" alt="" /></p>

<p>Jensen不等式应用于凹函数时，不等号方向反向。</p>

<h3 id="section">极大似然</h3>

<p>给定的训练样本是{x(1),…,x(m)}，样本间独立，我们想找到每个样例隐含的类别z，能使得p(x,z)最大。似然函数为：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_likelihood.png" alt="" /></p>

<p>极大化上面似然函数，需要对函数求导。但里面有”和的对数”，求导后形式会非常复杂（自己可以想象下log(f1(x)+ f2(x)+ f3(x)+…)复合函数的求导）。所以我们要做一个变换，如下图所示，经过这个变换后，”和的对数”变成了”对数的和”，这样计算起来就简单多了。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/jensen_transform.png" alt="" /></p>

<p>怎么变换来的呢？其中Q(z)表示隐含变量z的某种分布。由于f(x)=log(x)为凹函数，根据Jensen不等式有：f(E[X]) &gt;= E[f(X)]，即：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/jensen_transform2.png" alt="" /></p>

<p>OK，通过上面的变换，我们求得了似然函数的下届。我们可以优化这个下届，使其逼近似然函数(incomplete data)。</p>

<p>按照这个思路，我们要找到等式成立的条件。根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，这里得到：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/equality_condition.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/q_condition.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/p_condition.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/q_p_z_relation.png" alt="" /></p>

<table>
  <tbody>
    <tr>
      <td>从而，我们推导出：在固定其他参数后，Q(z)的计算公式就是z的后验概率。这一步也就是所谓的E步，求出Q函数，表示的是完全数据对数似然函数相对于隐变量的期望，而得到这个期望，也就是求出z的后验概率P(z</td>
      <td>x，θ)。</td>
    </tr>
  </tbody>
</table>

<p>M步呢，就是极大化Q函数，也就是优化θ的过程。</p>

<p>归纳下来，EM算法的基本步骤为：E步固定θ，优化Q；M步固定Q，优化θ。交替将极值推向最大。</p>

<h3 id="em-2">为什么EM是有效的?</h3>

<p>蓝线代表当前参数下的L函数，也就是目标函数的下界，E步的时候计算L函数，M步的时候通过重新计算θ得到L的最大值。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/em_prove1.png" alt="" /></p>

<h2 id="emplsa">EM于PLSA</h2>

<p>PLSA的图模型：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_graph_model.png" alt="" /></p>

<p>PLSA的生成过程：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_procedure.png" alt="" /></p>

<p>(di,wj)的联合分布为：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_formula1.png" alt="" /></p>

<p>PLSA的最大似然函数为：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_likelihood.png" alt="" /></p>

<p>注意上式中，第一项的完整形式为：
\(\sum<em>{i=1}^N{\sum</em>{j=1}^M{n(d_i,w_j) log(p(d_i))}}\)。</p>

<p>对于这样的包含”隐含变量”或者”缺失数据”的概率模型参数估计问题，我们采用EM算法。这两个概念是互相联系的，当我们的模型中有”隐含变量”时，我们会认为原始数据是”不完全的数据”，因为隐含变量的值无法观察到；反过来，当我们的数据incomplete时，我们可以通过增加隐含变量来对”缺失数据”建模。</p>

<p>EM算法的步骤是：</p>

<ul>
  <li>E步骤：Given当前估计的参数条件下，求隐含变量的后验概率。
an expectation (E) step where posterior probabilities are computed for the latent variables, based on the current estimates of the parameters。</li>
  <li>M步骤：最大化Complete data对数似然函数的期望，此时我们使用E步骤里计算的隐含变量的后验概率，得到新的参数值。
a maximization (M) step, where parameters are updated based on the so-called expected complete data log-likelihood which depends on the posterior probabilities computed in the E-step。</li>
</ul>

<p>两步迭代进行直到收敛。</p>

<p>这里是通过最大化”complete data”似然函数的期望，来最大化”incomplete data”的似然函数，以便得到求似然函数最大值更为简单的计算途径。</p>

<p>PLSA的E-Step：</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_e_step.png" alt="" /></p>

<p>PLSA的M-step，M-step的推导过程请参考下面的文献。</p>

<p><img src="https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/other/em/plsa_m_step.png" alt="" /></p>

<p>参考文献：</p>

<ul>
  <li>
    <p><a href="http://www.cs.bham.ac.uk/~pxt/IDA/plsa.pdf">Unsupervised Learning by Probabilistic Latent Semantic Analysis</a></p>
  </li>
  <li>
    <p><a href="http://blog.csdn.net/yangliuy/article/details/8330640">概率语言模型及其变形系列(1)-PLSA及EM算法</a></p>
  </li>
</ul>

<h2 id="emgmm">EM于GMM</h2>

<p>PRML第9章。</p>

<h2 id="emhmm">EM于HMM</h2>

<p>统计机器学习-HMM那一章节</p>

<h2 id="section-1">更多参考文献</h2>
<ul>
  <li><a href="https://github.com/zzbased/zzbased.github.com/blob/master/_posts/doc/我所理解的EM算法.docx">你所不知道的EM - by erikhu</a></li>
  <li><a href="http://blog.csdn.net/lansatiankongxxc/article/details/45646677">EM算法原理详解与高斯混合模型</a></li>
  <li><a href="http://blog.csdn.net/zouxy09/article/details/8537620">从最大似然到EM算法浅解</a></li>
</ul>


  
</section>


      </section>

      <footer>
        <p><small>Hosted on <a href="http://pages.github.com/">GitHub Pages</a> using the <a href="https://github.com/sodabrew/theme-dinky">Dinky theme</a> for <a href="http://jekyllbootstrap.com/">Jekyll Bootstrap</a></small></p>
      </footer>

    </div>
    <!--[if !IE]><script>fixScale(document);</script><!--<![endif]-->
  </body>
</html>

